{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second attempt Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import keras\n",
    "import sys, time, os, warnings \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"python {}\".format(sys.version))\n",
    "print(\"keras version {}\".format(keras.__version__))\n",
    "print(\"tensorflow version {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy']=\"http://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "os.environ['https_proxy']=\"https://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "\n",
    "# for TFBertModel\n",
    "PROXIES = {\n",
    "  \"http\": \"http://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "  \"https\": \"https://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flickr_image_dir = \"../Dataset/Flickr8k/Flicker8k_Dataset\"\n",
    "Flickr_text_dir = \"../Dataset/Flickr8k/Flickr8k.token.txt\"\n",
    "\n",
    "image_filenames = os.listdir(Flickr_image_dir)\n",
    "image_filenames = [x for x in image_filenames if \".npy\" not in x]\n",
    "print(\"The number of jpg flies in Flicker8k: {}\".format(len(image_filenames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the Flickr caption data\n",
    "file = open(Flickr_text_dir, 'r')\n",
    "text = file.read().strip().split('\\n')\n",
    "file.close()\n",
    "\n",
    "dataset = {}\n",
    "for line in text:\n",
    "    \n",
    "    # line: 1000268201_693b08cb0e.jpg#0\tA child in a pink...\n",
    "    image_path, caption = line.split('\\t')\n",
    "    image_path, path_num = image_path.split(\"#\")\n",
    "    \n",
    "    if image_path not in dataset:\n",
    "        dataset[image_path] = {\"captions\":[], \"tokens\":[]}\n",
    "    dataset[image_path][\"captions\"].append(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "\n",
    "def show_dataset_sample(n_sample=5):\n",
    "    \n",
    "    count = 1\n",
    "    fig = plt.figure(figsize=(10, 20))\n",
    "    \n",
    "    sample_images = random.sample(list(dataset), n_sample)\n",
    "    for image_path in sample_images:\n",
    "\n",
    "        captions = dataset[image_path][\"captions\"]\n",
    "        image_load = load_img(Flickr_image_dir + '/' + image_path, target_size=(224, 224, 3))\n",
    "\n",
    "        # Plot image\n",
    "        ax = fig.add_subplot(n_sample, 2, count, xticks=[], yticks=[])\n",
    "        ax.imshow(image_load)\n",
    "        count += 1\n",
    "\n",
    "        # Plot captions\n",
    "        ax = fig.add_subplot(n_sample, 2, count)\n",
    "        ax.plot()\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, len(captions))\n",
    "        ax.axis('off')    \n",
    "        for i, caption in enumerate(captions):\n",
    "            ax.text(0, i, caption, fontsize=16)\n",
    "        count += 1\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "show_dataset_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_extractor = keras.applications.xception.Xception(include_top=True, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.xception import preprocess_input\n",
    "\n",
    "PARAMS['image_shape'] = (299, 299, 3)\n",
    "PARAMS['image_feature_size'] = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_feature(image_path):\n",
    "    \n",
    "    # load an image from file\n",
    "    image = load_img(image_path, target_size=PARAMS['image_shape'])\n",
    "    image = img_to_array(image)\n",
    "    image = preprocess_input(image)\n",
    "    \n",
    "    feature = image_extractor.predict(image.reshape((1,) + image.shape[:3])).flatten()\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image_path in tqdm(dataset.keys()):\n",
    "    \n",
    "#     filename = Flickr_image_dir + \"/\" + image_path\n",
    "#     image_feature = get_image_feature(filename)\n",
    "#     np.save(filename + \".npy\", image_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = \"[CLS]\"\n",
    "END_TOKEN = \"[SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_end_seq_token(captions):\n",
    "    return [\"{} {} {}\".format(START_TOKEN, x, END_TOKEN) for x in captions]\n",
    "\n",
    "for key in dataset.keys():\n",
    "    dataset[key][\"captions\"] = add_start_end_seq_token(dataset[key][\"captions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare caption dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS[\"vocab_size\"] = 8000\n",
    "PARAMS[\"max_caption_length\"] = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions = [x for captions in dataset.values() for x in captions[\"captions\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=PARAMS[\"vocab_size\"])\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "\n",
    "dtexts = tokenizer.texts_to_sequences(all_captions)\n",
    "for key in tqdm(dataset.keys()):\n",
    "    dataset[key][\"tokens\"] = tokenizer.texts_to_sequences(dataset[key][\"captions\"])\n",
    "\n",
    "actual_size = len(tokenizer.word_index) + 1\n",
    "print(\"using {} of {} unique tokens ({:.2f} %)\".format(PARAMS[\"vocab_size\"], actual_size, PARAMS[\"vocab_size\"]/actual_size*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def build_paralel_dataset(dataset):\n",
    "    \n",
    "    images, captions, targets = [], [], []\n",
    "    \n",
    "    for key in tqdm(dataset.keys()):\n",
    "        \n",
    "        filename = Flickr_image_dir + \"/\" + key\n",
    "        image = np.load(filename + '.npy')\n",
    "        \n",
    "        tokens = dataset[key][\"tokens\"]\n",
    "        for token in tokens:\n",
    "            \n",
    "            for i in range(1, len(token)):\n",
    "                in_text, out_text = token[:i], token[i]\n",
    "                in_text = pad_sequences([in_text], \n",
    "                                   maxlen=PARAMS[\"max_caption_length\"],\n",
    "#                                    padding='post',\n",
    "                                   truncating='post').flatten()\n",
    "\n",
    "                images.append(image)\n",
    "                captions.append(in_text)\n",
    "                targets.append(out_text)\n",
    "                \n",
    "    return images, captions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# par_dt => paralel dataset\n",
    "par_dt_image_paths, par_dt_captions, par_dt_targets = build_paralel_dataset(dataset)\n",
    "assert(len(par_dt_image_paths) == len(par_dt_captions) == len(par_dt_targets))\n",
    "\n",
    "DATA_SIZE = len(par_dt_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS[\"batch_size\"] = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_dataset(image_path, caption, target):\n",
    "#     img_tensor = np.load(image_path.decode('utf-8') + '.npy')\n",
    "#     return img_tensor, caption, target\n",
    "\n",
    "\n",
    "# def create_dataset_object(par_dt_image_paths, par_dt_captions, par_dt_targets):\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((par_dt_image_paths, par_dt_captions, par_dt_targets))\n",
    "#     dataset = dataset.map(lambda item1, item2, item3: tf.numpy_function(\n",
    "#               load_dataset, [item1, item2, item3], [tf.float32, tf.int32, tf.int32]),\n",
    "#               num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#     dataset = dataset.batch(PARAMS[\"batch_size\"])\n",
    "#     dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "#     return dataset\n",
    "\n",
    "\n",
    "# tf_dataset = create_dataset_object(par_dt_image_paths, par_dt_captions, par_dt_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split dataset \n",
    "\n",
    "# TRAIN_SPLIT = 0.7\n",
    "# EVAL_SPLIT = 0.15\n",
    "# TEST_SPLIT = 0.15  # approx\n",
    "\n",
    "# n_batch = int(DATA_SIZE / PARAMS[\"batch_size\"]) + 1\n",
    "# n_train = int(n_batch * 0.7)\n",
    "# n_eval = int(n_batch * 0.15)\n",
    "# n_test = n_batch - (n_train + n_eval)\n",
    "\n",
    "# train_tf_dataset = tf_dataset.take(n_train)\n",
    "# eval_tf_dataset = tf_dataset.skip(n_train).take(n_eval)\n",
    "# test_tf_dataset = tf_dataset.skip(n_train + n_eval)\n",
    "\n",
    "\n",
    "# # \"\"\"\n",
    "# # tf_dataset => tuple of (image, captions, target)\n",
    "# # image   => (batch_size = 16, image_feature = 1000)\n",
    "# # caption => (batch_size = 16, max_length)\n",
    "# # caption => (batch_size = 16,)\n",
    "# # \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"train: {} batches, (total : {})\".format(n_train, n_train * PARAMS[\"batch_size\"]))\n",
    "# print(\"eval : {} batches, (total : {})\".format(n_eval, n_eval * PARAMS[\"batch_size\"]))\n",
    "# print(\"test : {} batches, (total : {} (aprx))\".format(n_test, n_test * PARAMS[\"batch_size\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS[\"word_embedding_size\"] = 64\n",
    "PARAMS[\"decoder_units\"] = 256\n",
    "PARAMS[\"encoder_units\"] = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "input_image = layers.Input(shape=(PARAMS[\"image_feature_size\"],))\n",
    "fimage = layers.Dense(PARAMS[\"encoder_units\"], activation='relu')(input_image)\n",
    "\n",
    "## sequence model\n",
    "input_caption = layers.Input(shape=(PARAMS[\"max_caption_length\"],))\n",
    "ftxt = layers.Embedding(PARAMS[\"vocab_size\"], output_dim=PARAMS[\"word_embedding_size\"], mask_zero=True)(input_caption)\n",
    "ftxt = layers.LSTM(PARAMS[\"decoder_units\"])(ftxt)\n",
    "\n",
    "## combined model for decoder\n",
    "decoder = layers.add([ftxt, fimage])\n",
    "decoder = layers.Dense(PARAMS[\"decoder_units\"], activation='relu')(decoder)\n",
    "output = layers.Dense(PARAMS[\"vocab_size\"], activation='softmax')(decoder)\n",
    "model = models.Model(inputs=[input_image, input_caption], outputs=output)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = 30000\n",
    "\n",
    "X_image_train, X_caption_train, y_train = (par_dt_image_paths[:train], par_dt_captions[:train], par_dt_targets[:train])\n",
    "X_image_train = tf.convert_to_tensor(X_image_train)\n",
    "X_caption_train = tf.convert_to_tensor(X_caption_train)\n",
    "y_train = tf.convert_to_tensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "hist = model.fit([X_image_train, X_caption_train], y_train, \n",
    "                  validation_data=([X_image_train, X_caption_train], y_train),\n",
    "                  epochs=5, verbose=1, \n",
    "                  batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in [\"loss\", \"val_loss\"]:\n",
    "    plt.plot(hist.history[label], label=label)\n",
    "    \n",
    "plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = dict([(index,word) for word, index in tokenizer.word_index.items()])\n",
    "def predict_caption(image):\n",
    "    '''\n",
    "    image.shape = (1,4462)\n",
    "    '''\n",
    "\n",
    "    in_text = 'startseq'\n",
    "\n",
    "    for iword in range(maxlen):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence],maxlen)\n",
    "        yhat = model.predict([image,sequence],verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        newword = index_word[yhat]\n",
    "        in_text += \" \" + newword\n",
    "        if newword == \"endseq\":\n",
    "            break\n",
    "    return(in_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "npic = 5\n",
    "npix = 224\n",
    "target_size = (npix,npix,3)\n",
    "\n",
    "count = 1\n",
    "fig = plt.figure(figsize=(10, 20))\n",
    "\n",
    "for jpgfnm, image_feature in zip(fnm_test[:npic],di_test[:npic]):\n",
    "    ## images \n",
    "    filename = dir_Flickr_jpg + '/' + jpgfnm\n",
    "    image_load = load_img(filename, target_size=target_size)\n",
    "    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n",
    "    ax.imshow(image_load)\n",
    "    count += 1\n",
    "\n",
    "    ## captions\n",
    "    caption = predict_caption(image_feature.reshape(1,len(image_feature)))\n",
    "    ax = fig.add_subplot(npic,2,count)\n",
    "    plt.axis('off')\n",
    "    ax.plot()\n",
    "    ax.set_xlim(0,1)\n",
    "    ax.set_ylim(0,1)\n",
    "    ax.text(0,0.5,caption,fontsize=20)\n",
    "    count += 1\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
