{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second attempt Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    'image_shape': (299, 299, 3),\n",
    "    'image_feature_size': 1000,\n",
    "    'vocab_size': 7000,\n",
    "    'max_caption_length': 25,\n",
    "    'batch_size': 128,\n",
    "    'word_embedding_size': 768,\n",
    "    'decoder_units': 256,\n",
    "    'encoder_units': 256\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import keras\n",
    "import sys, time, os, warnings \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"python {}\".format(sys.version))\n",
    "print(\"keras version {}\".format(keras.__version__))\n",
    "print(\"tensorflow version {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy']=\"http://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "os.environ['https_proxy']=\"https://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "\n",
    "# for TFBertModel\n",
    "PROXIES = {\n",
    "  \"http\": \"http://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "  \"https\": \"https://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flickr_image_dir = \"../Dataset/Flickr8k/Flicker8k_Dataset\"\n",
    "# Flickr_text_dir = \"../Dataset/Flickr8k/Flickr8k.token.txt\"\n",
    "\n",
    "# image_filenames = os.listdir(Flickr_image_dir)\n",
    "# image_filenames = [x for x in image_filenames if \".npy\" not in x]\n",
    "# print(\"The number of jpg flies in Flicker8k: {}\".format(len(image_filenames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## read in the Flickr caption data\n",
    "# file = open(Flickr_text_dir, 'r')\n",
    "# text = file.read().strip().split('\\n')\n",
    "# file.close()\n",
    "\n",
    "# dataset = {}\n",
    "# for line in text:\n",
    "    \n",
    "#     # line: 1000268201_693b08cb0e.jpg#0\tA child in a pink...\n",
    "#     image_path, caption = line.split('\\t')\n",
    "#     image_path, path_num = image_path.split(\"#\")\n",
    "    \n",
    "#     if image_path not in dataset:\n",
    "#         dataset[image_path] = {\"captions\":[], \"tokens\":[]}\n",
    "#     dataset[image_path][\"captions\"].append(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "\n",
    "# def show_dataset_sample(n_sample=5):\n",
    "    \n",
    "#     count = 1\n",
    "#     fig = plt.figure(figsize=(10, 20))\n",
    "    \n",
    "#     sample_images = random.sample(list(dataset), n_sample)\n",
    "#     for image_path in sample_images:\n",
    "\n",
    "#         captions = dataset[image_path][\"captions\"]\n",
    "#         image_load = load_img(Flickr_image_dir + '/' + image_path, target_size=(224, 224, 3))\n",
    "\n",
    "#         # Plot image\n",
    "#         ax = fig.add_subplot(n_sample, 2, count, xticks=[], yticks=[])\n",
    "#         ax.imshow(image_load)\n",
    "#         count += 1\n",
    "\n",
    "#         # Plot captions\n",
    "#         ax = fig.add_subplot(n_sample, 2, count)\n",
    "#         ax.plot()\n",
    "#         ax.set_xlim(0, 1)\n",
    "#         ax.set_ylim(0, len(captions))\n",
    "#         ax.axis('off')    \n",
    "#         for i, caption in enumerate(captions):\n",
    "#             ax.text(0, i, caption, fontsize=12)\n",
    "#         count += 1\n",
    "\n",
    "#     plt.show()\n",
    "    \n",
    "\n",
    "# show_dataset_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# image_extractor = keras.applications.xception.Xception(include_top=True, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.image import load_img, img_to_array\n",
    "# from keras.applications.xception import preprocess_input\n",
    "\n",
    "# PARAMS['image_shape'] = (299, 299, 3)\n",
    "# PARAMS['image_feature_size'] = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_image_feature(image_path):\n",
    "    \n",
    "#     # load an image from file\n",
    "#     image = load_img(image_path, target_size=PARAMS['image_shape'])\n",
    "#     image = img_to_array(image)\n",
    "#     image = preprocess_input(image)\n",
    "    \n",
    "#     feature = image_extractor.predict(image.reshape((1,) + image.shape[:3])).flatten()\n",
    "    \n",
    "#     return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image_path in tqdm(dataset.keys()):\n",
    "    \n",
    "#     filename = Flickr_image_dir + \"/\" + image_path\n",
    "#     image_feature = get_image_feature(filename)\n",
    "#     np.save(filename + \".npy\", image_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = \"[CLS]\"\n",
    "END_TOKEN = \"[SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_start_end_seq_token(captions):\n",
    "#     return [\"{} {} {}\".format(START_TOKEN, x, END_TOKEN) for x in captions]\n",
    "\n",
    "# for key in dataset.keys():\n",
    "#     dataset[key][\"captions\"] = add_start_end_seq_token(dataset[key][\"captions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare caption dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS[\"max_caption_length\"] = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_captions = [x for captions in dataset.values() for x in captions[\"captions\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# tokenizer = Tokenizer(nb_words=PARAMS[\"vocab_size\"])\n",
    "# tokenizer.fit_on_texts(all_captions)\n",
    "\n",
    "# dtexts = tokenizer.texts_to_sequences(all_captions)\n",
    "# for key in tqdm(dataset.keys()):\n",
    "#     dataset[key][\"tokens\"] = tokenizer.texts_to_sequences(dataset[key][\"captions\"])\n",
    "\n",
    "# actual_size = len(tokenizer.word_index) + 1\n",
    "# print(\"using {} of {} unique tokens ({:.2f} %)\".format(PARAMS[\"vocab_size\"], actual_size, PARAMS[\"vocab_size\"]/actual_size*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS[\"tokenizer\"] = \"BERT\"\n",
    "PARAMS[\"use_mapping\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BertTokenizerWrapper(BertTokenizer):\n",
    "    \n",
    "#     def use_custom_mapping(self, use_mapping=True, vocab_size=5000):\n",
    "        \n",
    "#         self.use_mapping = use_mapping\n",
    "#         self.cust_vocab_size = vocab_size\n",
    "#         self.mapping_initialized = False\n",
    "\n",
    "        \n",
    "#     def texts_to_sequences(self, texts):\n",
    "#         \"\"\"\n",
    "#         convert batch texts into custom indexed version\n",
    "#         eg: ['an apple', 'two person']\n",
    "#         output: [[1037,17260], [2083, 2711]] \n",
    "#         \"\"\"\n",
    "        \n",
    "#         return [self.convert_tokens_to_ids(self.tokenize(x)) for x in texts]\n",
    "    \n",
    "        \n",
    "#     def convert_tokens_to_ids(self, tokens):\n",
    "        \n",
    "#         bert_ids = self._get_bert_ids(tokens)\n",
    "        \n",
    "#         if not self.use_mapping:\n",
    "#             return bert_ids\n",
    "        \n",
    "#         if not self.mapping_initialized:\n",
    "#             raise Exception(\"mapping not initialized\")\n",
    "        \n",
    "#         return self._convert_bert_id_to_custom_id(bert_ids)\n",
    "        \n",
    "        \n",
    "#     def convert_ids_to_tokens(self, token_ids):\n",
    "        \n",
    "#         if self.use_mapping and self.mapping_initialized:\n",
    "#             bert_ids = self._convert_custom_id_to_bert_id(token_ids)\n",
    "#         else:\n",
    "#             bert_ids = token_ids\n",
    "            \n",
    "#         bert_tokens = super().convert_ids_to_tokens(bert_ids)\n",
    "#         return bert_tokens\n",
    "    \n",
    "    \n",
    "#     def initialize_custom_mapping(self, texts):\n",
    "        \n",
    "#         bert_ids = [self._get_bert_ids(self.tokenize(x)) for x in tqdm(texts)]\n",
    "#         self._build_occurence_table(bert_ids)\n",
    "#         self._build_custom_mapping_table()\n",
    "#         self.mapping_initialized = True\n",
    "        \n",
    "        \n",
    "#     def _get_bert_ids(self, tokens):\n",
    "#         return super().convert_tokens_to_ids(tokens)\n",
    "    \n",
    "        \n",
    "#     def _build_occurence_table(self, tokenized_captions):\n",
    "#         \"\"\"\n",
    "#         build dict of token frequency\n",
    "#         \"\"\"\n",
    "        \n",
    "#         self.occurence_table = {}\n",
    "#         for caption in tqdm(tokenized_captions):\n",
    "#             for token in caption:\n",
    "#                 if token not in self.occurence_table:\n",
    "#                     self.occurence_table[token] = 0\n",
    "#                 self.occurence_table[token] += 1\n",
    "                \n",
    "    \n",
    "#     def _build_custom_mapping_table(self):\n",
    "        \n",
    "#         _special_token = ['[UNK]', '[PAD]']\n",
    "#         _actual_vocab_size = self.cust_vocab_size - len(_special_token)\n",
    "        \n",
    "#         sorted_occurence = {k: v for k, v in sorted(\n",
    "#             self.occurence_table.items(), reverse=True, key=lambda item: item[1]\n",
    "#         )}\n",
    "        \n",
    "#         used_tokens = sorted(list(sorted_occurence)[:_actual_vocab_size])\n",
    "#         mapping_size = min(len(used_tokens), _actual_vocab_size)\n",
    "        \n",
    "#         _bert_pad = 0\n",
    "#         _bert_oov = 100\n",
    "#         self._custom_pad = 0\n",
    "#         self._custom_oov = mapping_size + 1\n",
    "        \n",
    "#         self.bert_id_to_custom_id = {\n",
    "#             _bert_pad: self._custom_pad, \n",
    "#             _bert_oov: self._custom_oov\n",
    "#         }\n",
    "#         self.custom_id_to_bert_id = {\n",
    "#             self._custom_pad: _bert_pad, \n",
    "#             self._custom_oov: _bert_oov\n",
    "#         }\n",
    "        \n",
    "#         for i in range(0, mapping_size):\n",
    "#             bert_token = used_tokens[i]\n",
    "#             self.bert_id_to_custom_id[bert_token] = i + 1    \n",
    "#             self.custom_id_to_bert_id[i + 1] = bert_token\n",
    "            \n",
    "#         print(\"Vocab contains {0} / {1} unique tokens ({2:.2f} %)\".format(\n",
    "#             len(used_tokens) + 2,\\\n",
    "#             len(sorted_occurence),\\\n",
    "#             (len(used_tokens) / len(sorted_occurence) * 100)\n",
    "#         ))\n",
    "        \n",
    "#         sorted_occurence_count = list(sorted_occurence.values())\n",
    "#         used_tokens_count = sum(sorted_occurence_count[:_actual_vocab_size])\n",
    "#         total_tokens_count = sum(sorted_occurence_count)\n",
    "        \n",
    "#         print(\"Using {0} / {1} tokens available ({2:.2f} %)\".format(\n",
    "#             used_tokens_count,\\\n",
    "#             total_tokens_count,\\\n",
    "#             (used_tokens_count / total_tokens_count * 100)\n",
    "#         ))        \n",
    "        \n",
    "#     def _convert_bert_id_to_custom_id(self, token_ids):\n",
    "#         return [self.bert_id_to_custom_id[x] if x in self.bert_id_to_custom_id else self._custom_oov for x in token_ids]\n",
    "    \n",
    "#     def _convert_custom_id_to_bert_id(self, token_ids):\n",
    "#         return [self.custom_id_to_bert_id[x] for x in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TokenizerWrapper(Tokenizer):\n",
    "    \n",
    "#     def convert_tokens_to_ids(self, tokens):\n",
    "#         return [self.word_index[x] for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_tokenizer(tokenizer_type, use_mapping, vocab_size):\n",
    "    \n",
    "#     # Load pre-trained BERT tokenizer (vocabulary)\n",
    "#     if tokenizer_type == \"BERT\" :\n",
    "#         tokenizer = BertTokenizerWrapper.from_pretrained('bert-base-uncased')\n",
    "#         tokenizer.use_custom_mapping(use_mapping, vocab_size)\n",
    "\n",
    "#     # use default keras tokenizer\n",
    "#     else : \n",
    "#         tokenizer = TokenizerWrapper(num_words=vocab_size, oov_token=\"[UNK]\")\n",
    "#         tokenizer.fit_on_texts(train_captions)    \n",
    "#         tokenizer.word_index['[PAD]'] = 0\n",
    "#         tokenizer.index_word[0] = '[PAD]'\n",
    "        \n",
    "#     return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caption_tokenizer = get_tokenizer(\n",
    "#     tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "#     use_mapping=False,\n",
    "#     vocab_size=0\n",
    "# )\n",
    "\n",
    "# target_tokenizer = get_tokenizer(\n",
    "#     tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "#     use_mapping=PARAMS[\"use_mapping\"],\n",
    "#     vocab_size=PARAMS[\"vocab_size\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_tokenizer.initialize_custom_mapping(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for key in tqdm(dataset.keys()):\n",
    "#     dataset[key][\"tokens\"] = caption_tokenizer.texts_to_sequences(dataset[key][\"captions\"])\n",
    "#     dataset[key][\"target_tokens\"] = target_tokenizer.texts_to_sequences(dataset[key][\"captions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# def build_paralel_dataset(dataset):\n",
    "    \n",
    "#     images, captions, targets = [], [], []\n",
    "    \n",
    "#     for key in tqdm(dataset.keys()):\n",
    "        \n",
    "#         filename = Flickr_image_dir + \"/\" + key\n",
    "#         image = np.load(filename + '.npy')\n",
    "        \n",
    "#         tokens = dataset[key][\"tokens\"]\n",
    "#         target_tokens = dataset[key][\"target_tokens\"]\n",
    "        \n",
    "#         for i in range(len(tokens)):\n",
    "            \n",
    "#             token = tokens[i]\n",
    "#             target_token = target_tokens[i]\n",
    "            \n",
    "#             for j in range(1, len(token)):\n",
    "                \n",
    "#                 in_text = pad_sequences([token[:j]], maxlen=PARAMS[\"max_caption_length\"], padding='post', truncating='post').flatten()\n",
    "#                 out_text = pad_sequences([target_token[1:j+1]], maxlen=PARAMS[\"max_caption_length\"], padding='post', truncating='post').flatten()\n",
    "\n",
    "#                 images.append(image)\n",
    "#                 captions.append(in_text)\n",
    "#                 targets.append(out_text)\n",
    "                \n",
    "#     return images, captions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # par_dt => paralel dataset\n",
    "# par_dt_image_paths, par_dt_captions, par_dt_targets = build_paralel_dataset(dataset)\n",
    "# assert(len(par_dt_image_paths) == len(par_dt_captions) == len(par_dt_targets))\n",
    "\n",
    "# DATA_SIZE = len(par_dt_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_image_train, X_image_eval, X_caption_train, X_caption_eval, y_train, y_eval = train_test_split(par_dt_image_paths, par_dt_captions, par_dt_targets, test_size=0.2, random_state=42)\n",
    "# X_image_train, X_image_test, X_caption_train, X_caption_test, y_train, y_test = train_test_split(X_image_train, X_caption_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# print(\"train : {})\".format(len(X_image_train)))\n",
    "# print(\"eval  : {})\".format(len(X_image_eval)))\n",
    "# print(\"test  : {})\".format(len(X_image_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_image_train = np.array(X_image_train)\n",
    "# X_caption_train = np.array(X_caption_train)\n",
    "# y_train = np.array(y_train)\n",
    "\n",
    "# X_image_eval = np.array(X_image_eval)\n",
    "# X_caption_eval = np.array(X_caption_eval)\n",
    "# y_eval = np.array(y_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, LeakyReLU, \\\n",
    "                                    Embedding, Bidirectional, LSTM, Concatenate, \\\n",
    "                                    GlobalAveragePooling1D, Reshape, Flatten, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS[\"decoder_units\"] = 256\n",
    "PARAMS[\"encoder_units\"] = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from transformers import TFBertModel\n",
    "\n",
    "\n",
    "class BertEmbedding(layers.Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BertEmbedding, self).__init__(**kwargs)\n",
    "        self.embedding = TFBertModel.from_pretrained('bert-base-uncased', proxies=PROXIES)\n",
    "        self.embedding.trainable = False\n",
    "        self.embedding_dim = self.embedding.config.hidden_size\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        is_sentence = tf.cast((inputs == 0), tf.int32)\n",
    "        hidden_states, _ = self.embedding(inputs=inputs, token_type_ids=is_sentence)\n",
    "        return hidden_states\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(BertEmbedding, self).get_config()\n",
    "#         config.update({'embedding_dim': self.embedding_dim})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"\n",
    "    Calculate the attention weights.\n",
    "    \n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, **kwargs):\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super(MultiHeadAttention, self).get_config()\n",
    "        config.update({\n",
    "            'num_heads': self.num_heads,\n",
    "            'd_model': self.d_model,\n",
    "        })\n",
    "        return config\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        \n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.rate = rate\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(EncoderLayer, self).get_config()\n",
    "        config.update({\n",
    "            'd_model': self.d_model,\n",
    "            'num_heads': self.num_heads,\n",
    "            'dff': self.dff,\n",
    "            'rate': self.rate,\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  \n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.rate = rate\n",
    "        \n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(DecoderLayer, self).get_config()\n",
    "        config.update({\n",
    "            'd_model': self.d_model,\n",
    "            'num_heads': self.num_heads,\n",
    "            'dff': self.dff,\n",
    "            'rate': self.rate,\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, embedding_dim, vocab_size, maximum_position_encoding, embedding_type=\"BERT\", **kwargs):\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.maximum_position_encoding = maximum_position_encoding\n",
    "        self.embedding_type = embedding_type\n",
    "        \n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, embedding_dim)       \n",
    "        if self.embedding_type == \"BERT\":\n",
    "            self.embedding = BertEmbedding()\n",
    "        else:\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(TransformerEmbedding, self).get_config()\n",
    "        config.update({\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'maximum_position_encoding': self.maximum_position_encoding,\n",
    "            'embedding_type': self.embedding_type,\n",
    "        })\n",
    "        return config            \n",
    "                 \n",
    "    def call(self, x):\n",
    "        \n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout, LayerNormalization\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, num_layers, num_heads, dff, d_model=None, rate=0.1, **kwargs):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.rate = rate\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        if self.d_model is None:\n",
    "            self.d_model = input_shape[-1]\n",
    "        \n",
    "        self.enc_layers = [EncoderLayer(d_model=self.d_model, num_heads=self.num_heads, \n",
    "                                        dff=self.dff, rate=self.rate) for _ in range(self.num_layers)]\n",
    "        \n",
    "        self.dropout = Dropout(self.rate)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(Encoder, self).get_config()\n",
    "        config.update({\n",
    "            'd_model': self.d_model,\n",
    "            'num_layers': self.num_layers,\n",
    "            'num_heads': self.num_heads,\n",
    "            'dff': self.dff,\n",
    "            'rate': self.rate,\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout, LayerNormalization\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, num_heads, dff, d_model=None, rate=0.1, **kwargs):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.rate = rate\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        if self.d_model is None:\n",
    "            self.d_model = input_shape[-1]\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model=self.d_model, num_heads=self.num_heads, \n",
    "                                        dff=self.dff, rate=self.rate) for _ in range(self.num_layers)]\n",
    "        \n",
    "        self.layernorms = [LayerNormalization(epsilon=1e-6) for _ in range(self.num_layers)]\n",
    "        self.dropout = Dropout(self.rate)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(Decoder, self).get_config()\n",
    "        config.update({\n",
    "            'd_model': self.d_model,\n",
    "            'num_layers': self.num_layers,\n",
    "            'num_heads': self.num_heads,\n",
    "            'dff': self.dff,\n",
    "            'rate': self.rate,\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "        prev_x = x * 0\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            x = self.layernorms[i](x + prev_x)\n",
    "            prev_x = x\n",
    "            \n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersMask(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,  **kwargs):\n",
    "        super(TransformersMask, self).__init__()\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super(TransformersMask, self).get_config()\n",
    "        return config  \n",
    "    \n",
    "    def _create_padding_mask(self, decoder_input):    \n",
    "        seq = tf.cast(tf.math.equal(decoder_input, 0), tf.float32)\n",
    "        return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "    \n",
    "    def _create_look_ahead_mask(self, size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask # (seq_len, seq_len)\n",
    "    \n",
    "    def call(self, encoder_input, decoder_input):\n",
    "        \n",
    "        # encoder_input = input\n",
    "        # decoder_input = target\n",
    "        \n",
    "        enc_padding_mask = self._create_padding_mask(encoder_input)\n",
    "        dec_padding_mask = self._create_padding_mask(encoder_input)\n",
    "        \n",
    "        look_ahead_mask = self._create_look_ahead_mask(tf.shape(decoder_input)[1])\n",
    "        dec_target_padding_mask = self._create_padding_mask(decoder_input)\n",
    "        combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "        return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "def MaskedSparseCategoricalCrossentropy(layer):\n",
    "\n",
    "    # Create a loss function that adds the MSE loss to the mean of all squared activations of a specific layer\n",
    "    def loss(y_true, y_pred):\n",
    "        \n",
    "        cce = K.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "        mask = tf.cast(y_true != 0, \"float32\")\n",
    "        batch_loss = K.sum(mask * cce, axis=1)\n",
    "        \n",
    "        return K.mean(batch_loss)\n",
    "   \n",
    "    # Return a function\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, LeakyReLU, \\\n",
    "                                    Embedding, Bidirectional, LSTM, Concatenate, \\\n",
    "                                    GlobalAveragePooling1D, Reshape, Flatten, Masking\n",
    "\n",
    "input_image = Input(shape=(PARAMS[\"image_feature_size\"],))\n",
    "input_caption = Input(shape=(PARAMS[\"max_caption_length\"],), dtype=\"int32\")\n",
    "\n",
    "caption_mask_layer = Masking(mask_value=0)\n",
    "\n",
    "caption_embedding_layer = TransformerEmbedding(embedding_dim=768, embedding_type=\"BERT\", vocab_size=PARAMS[\"vocab_size\"], maximum_position_encoding=1024)\n",
    "\n",
    "# encoder_layers = Encoder(num_layers=3, d_model=1000, num_heads=8, dff=1024, rate=0.1)\n",
    "decoder_layers = Decoder(num_layers=6, d_model=768, num_heads=8, dff=1024, rate=0.1)\n",
    "\n",
    "# =========================================== #\n",
    "\n",
    "masked_input_caption = caption_mask_layer(input_caption)\n",
    "\n",
    "emb_caption = caption_embedding_layer(masked_input_caption)\n",
    "\n",
    "# // force mask to have shape (batchsize, 1) instead of (batchsize, 768 or 1000)\n",
    "expand_image = tf.expand_dims(input_image, axis=1)\n",
    "dummy_image = tf.reduce_sum(expand_image, axis=-1)\n",
    "\n",
    "enc_padding_mask, combined_mask, dec_padding_mask = TransformersMask()(dummy_image, input_caption)\n",
    "print(\"enc_padding_mask\", enc_padding_mask.shape)\n",
    "print(\"combined_mask\", combined_mask.shape)\n",
    "print(\"dec_padding_mask\", dec_padding_mask.shape)\n",
    "\n",
    "# enc_output = encoder_layers(expand_image, training=True, mask=enc_padding_mask)\n",
    "dec_output, attention_weights = decoder_layers(emb_caption, input_image, training=True, \n",
    "    look_ahead_mask=combined_mask, padding_mask=None)\n",
    "\n",
    "output = Dense(PARAMS[\"vocab_size\"], activation='softmax')(dec_output)    \n",
    "model = Model(inputs=[input_image, input_caption], outputs=output)\n",
    "\n",
    "loss = MaskedSparseCategoricalCrossentropy(output)\n",
    "model.compile(loss=loss, optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "hist = model.fit([X_image_train, X_caption_train], y_train, \n",
    "                  validation_data=([X_image_eval, X_caption_eval], y_eval),\n",
    "                  epochs=10, verbose=1, \n",
    "                  batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"temp.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"temp.h5\", custom_objects={\n",
    "    'BertEmbedding': BertEmbedding,\n",
    "    'MultiHeadAttention': MultiHeadAttention,\n",
    "    'EncoderLayer': EncoderLayer,\n",
    "    'DecoderLayer': DecoderLayer,\n",
    "    'TransformerEmbedding': TransformerEmbedding,\n",
    "    'Encoder': Encoder,\n",
    "    'Decoder': Decoder,\n",
    "    'TransformersMask': TransformersMask,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in [\"loss\", \"val_loss\"]:\n",
    "    plt.plot(hist.history[label], label=label)\n",
    "    \n",
    "plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_prediction(n_sample=5):\n",
    "\n",
    "    count = 1\n",
    "    fig = plt.figure(figsize=(10, 20))\n",
    "\n",
    "    sample_images = random.sample(list(dataset), n_sample)\n",
    "    for image_path in tqdm(sample_images):\n",
    "\n",
    "        ## images \n",
    "        filename = Flickr_image_dir + '/' + image_path\n",
    "        image_load = load_img(filename, target_size=(224, 224, 3))\n",
    "        \n",
    "        ax = fig.add_subplot(n_sample, 2, count, xticks=[], yticks=[])\n",
    "        ax.imshow(image_load)\n",
    "        count += 1\n",
    "\n",
    "        ## captions\n",
    "        predict_input = np.load(filename + '.npy')\n",
    "        caption = predict_caption(predict_input)\n",
    "        \n",
    "        ax = fig.add_subplot(n_sample, 2, count)\n",
    "        plt.axis('off')\n",
    "        ax.plot()\n",
    "        ax.set_xlim(0,1)\n",
    "        ax.set_ylim(0,1)\n",
    "        ax.text(0, 0.5, caption, fontsize=12)\n",
    "        count += 1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, log\n",
    "from numpy import argmax\n",
    " \n",
    "\n",
    "def beam_search_decoder(data, k, return_best=True):\n",
    "    \n",
    "    sequences = [[list(), 1.0]]\n",
    "    \n",
    "    # walk over each step in sequence\n",
    "    for row in (data):\n",
    "        \n",
    "        all_candidates = list()\n",
    "        \n",
    "        # expand each current candidate\n",
    "        for i in range(len(sequences)):\n",
    "            \n",
    "            seq, score = sequences[i]\n",
    "            for j in range(len(row)):\n",
    "                \n",
    "                candidate = [seq + [j], score * -log(row[j])]\n",
    "                all_candidates.append(candidate)\n",
    "                \n",
    "        # order all candidates by score\n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
    "        \n",
    "        # select k best\n",
    "        sequences = ordered[:k]\n",
    "        \n",
    "    if return_best:\n",
    "        return ordered[0]\n",
    "        \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_search_decoder(predictions, k=3):\n",
    "    \n",
    "    # shape = batch_size, seq_len, vocab_size\n",
    "    shape = predictions.shape\n",
    "    \n",
    "    predictions = tf.reshape(predictions, (-1, shape[-1]))\n",
    "    \n",
    "    # sampled_proba & sampled_ids => (batch_size * seq_len, sampling_k)\n",
    "    sampled_proba, sampled_ids = tf.math.top_k(predictions, k)\n",
    "\n",
    "    # chosen_sampled_col => (batch_size * seq_len, )\n",
    "    chosen_sampled_col = tf.squeeze(tf.random.categorical(sampled_proba, 1))\n",
    "    chosen_sampled_col = tf.reshape(chosen_sampled_col, (-1,))\n",
    "\n",
    "    # create row idx to zip with chosen_sampled_col\n",
    "    row_idx = tf.range(predictions.shape[0], dtype=chosen_sampled_col.dtype)\n",
    "    row_col_idx = tf.stack([row_idx, chosen_sampled_col], axis=1)\n",
    "\n",
    "    # predicted_ids => (batch_size, seq_len, )\n",
    "    predicted_ids = tf.gather_nd(sampled_ids, row_col_idx)\n",
    "    predicted_ids = tf.reshape(predicted_ids, shape[:-1])\n",
    "    \n",
    "    return predicted_ids.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = caption_tokenizer.texts_to_sequences(['[CLS]'])[0][0]\n",
    "END_TOKEN = caption_tokenizer.texts_to_sequences(['[SEP]'])[0][0]\n",
    "\n",
    "\n",
    "def choose_word_ids(vocab_proba, strategy=\"greedy\", seq_index=0):\n",
    "    \n",
    "    vocab_proba = vocab_proba[:, :seq_index + 1]\n",
    "    \n",
    "    if strategy == \"beam_search\":\n",
    "        predicted_ids, _ = beam_search_decoder(vocab_proba[0], 3, return_best=True)\n",
    "    \n",
    "    elif strategy == \"sample\":\n",
    "        predicted_ids = sampling_search_decoder(vocab_proba[0], k=3)\n",
    "    \n",
    "    elif strategy == \"greedy\":\n",
    "        predicted_ids = np.argmax(vocab_proba[0], axis=-1)\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"not implemented\")\n",
    "        \n",
    "    bert_ids = target_tokenizer._convert_custom_id_to_bert_id(predicted_ids)\n",
    "    return bert_ids\n",
    "    \n",
    "    \n",
    "def append_choosen_word(sequence, predicted_ids, strategy=\"append\", seq_index=0):\n",
    "    \n",
    "    if strategy == \"replace\":\n",
    "        sequence = [START_TOKEN] + predicted_ids\n",
    "        \n",
    "    elif strategy == \"append\":\n",
    "        sequence = np.append(sequence, predicted_ids[seq_index])\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"not implemented\")\n",
    "    \n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_caption(image):\n",
    "    '''\n",
    "    image.shape = (1,4462)\n",
    "    '''\n",
    "    \n",
    "    image = np.array([image])\n",
    "    sequence = [START_TOKEN]\n",
    "    \n",
    "    for i in range(PARAMS[\"max_caption_length\"]):\n",
    "        \n",
    "        input_caption = np.array([sequence])\n",
    "        input_caption = pad_sequences(input_caption, PARAMS[\"max_caption_length\"], padding=\"post\") \n",
    "        \n",
    "        yhat = model.predict([image, input_caption], verbose=0)\n",
    "        \n",
    "        choosen_ids = choose_word_ids(yhat, strategy=\"beam_search\", seq_index=i)\n",
    "        sequence = append_choosen_word(sequence, choosen_ids, strategy=\"append\", seq_index=i)\n",
    "        \n",
    "        if sequence[-1] == END_TOKEN:\n",
    "            break\n",
    "            \n",
    "    prediction_text = caption_tokenizer.convert_ids_to_tokens(sequence)\n",
    "    prediction_text = \" \".join(prediction_text)\n",
    "    return(prediction_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_prediction(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_image_test = np.array(X_image_test)\n",
    "X_caption_test = np.array(X_caption_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "image = X_image_test[220:233]\n",
    "input_caption = X_caption_test[220:233]\n",
    "ytrue = y_test[220:233]\n",
    "\n",
    "yhat = model.predict([image, input_caption], verbose=0)\n",
    "yhat = np.argmax(yhat, axis=-1)\n",
    "\n",
    "bert_ids = [target_tokenizer.convert_ids_to_tokens(x) for x in yhat]\n",
    "bert_ids = [\" \".join(x) for x in bert_ids]\n",
    "\n",
    "true_pred = [target_tokenizer.convert_ids_to_tokens(x) for x in ytrue]\n",
    "true_pred = [\" \".join(x) for x in true_pred]\n",
    "\n",
    "\n",
    "for x,y in zip(bert_ids, true_pred):\n",
    "    print(\"pred: \", x)\n",
    "    print(\"true: \", y)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_caption(image):\n",
    "#     '''\n",
    "#     image.shape = (1,4462)\n",
    "#     '''\n",
    "    \n",
    "#     image = np.array([image])\n",
    "    \n",
    "#     START_TOKEN = caption_tokenizer.texts_to_sequences(['[CLS]'])[0]\n",
    "#     sequence = START_TOKEN\n",
    "    \n",
    "#     for i in tqdm(range(PARAMS[\"max_caption_length\"])):\n",
    "        \n",
    "#         input_caption = np.array([sequence])\n",
    "#         input_caption = pad_sequences(input_caption, PARAMS[\"max_caption_length\"], padding=\"post\") \n",
    "        \n",
    "#         yhat = model.predict([image, input_caption], verbose=0)\n",
    "        \n",
    "#         # ------------------------\n",
    "#         yhat = yhat[:, :i+1]\n",
    "        \n",
    "# #         # beam search\n",
    "# #         predicted_ids, _ = beam_search_decoder(yhat[0], 3, return_best=True)\n",
    "# #         bert_ids = target_tokenizer._convert_custom_id_to_bert_id(predicted_ids)\n",
    "        \n",
    "#         # greedy\n",
    "#         predicted_ids = np.argmax(yhat, axis=-1).squeeze(0)\n",
    "#         bert_ids = target_tokenizer._convert_custom_id_to_bert_id(predicted_ids)\n",
    "        \n",
    "#         sequence = START_TOKEN + bert_ids\n",
    "#         #--------------------------------\n",
    "        \n",
    "#         # check if break\n",
    "#         predicted_word = caption_tokenizer.convert_ids_to_tokens([bert_ids[i]])\n",
    "#         if predicted_word[0] == \"[SEP]\":\n",
    "#             break\n",
    "            \n",
    "#     prediction_text = caption_tokenizer.convert_ids_to_tokens(sequence)\n",
    "#     prediction_text = \" \".join(prediction_text)\n",
    "#     return(prediction_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_caption(image):\n",
    "#     '''\n",
    "#     image.shape = (1,4462)\n",
    "#     '''\n",
    "    \n",
    "#     image = np.array([image])\n",
    "    \n",
    "#     START_TOKEN = caption_tokenizer.texts_to_sequences(['[CLS]'])[0]\n",
    "#     sequence = START_TOKEN\n",
    "    \n",
    "#     for i in range(PARAMS[\"max_caption_length\"]):\n",
    "        \n",
    "#         input_caption = np.array([sequence])\n",
    "#         input_caption = pad_sequences(input_caption, PARAMS[\"max_caption_length\"], padding=\"post\")\n",
    "        \n",
    "#         yhat = model.predict([image, input_caption], verbose=0)\n",
    "        \n",
    "#         #--------------------------------\n",
    "#         # predicted_ids => (batch_size, seq_len)\n",
    "#         predicted_ids = choose_predicted_id(yhat, strategy=\"sample\", sampling_k=3)\n",
    "    \n",
    "#         predicted_id = predicted_ids[0][i].numpy()\n",
    "        \n",
    "#         predicted_id = target_tokenizer._convert_custom_id_to_bert_id([predicted_id])\n",
    "\n",
    "#         sequence = np.append(sequence, predicted_id)\n",
    "#         #--------------------------------\n",
    "        \n",
    "#         predicted_word = caption_tokenizer.convert_ids_to_tokens(predicted_id)\n",
    "#         if predicted_word[0] == \"[SEP]\":\n",
    "#             break\n",
    "            \n",
    "#     prediction_text = caption_tokenizer.convert_ids_to_tokens(sequence)\n",
    "#     prediction_text = \" \".join(prediction_text)\n",
    "#     return(prediction_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
